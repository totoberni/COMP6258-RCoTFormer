#!/bin/bash
################################################################################
# Slurm Job Script Template for Iridis X
#
# HOW TO USE:
#   1. Copy this file into your package directory:
#        cp job.slurm.example <your_package>/job.slurm
#   2. Replace every <PKG_NAME> below with your package directory name
#   3. Adjust resources (GPU type, memory, time) for your workload
#   4. Submit from the project root (~/dpdl/):
#        sbatch <your_package>/job.slurm
#
# FINDING CLUSTER INFORMATION:
#   sinfo -s                              # List all partitions and node counts
#   sinfo -p gpu --Node --long            # Detailed GPU node info
#   sacctmgr show assoc user=$USER        # Your account name (--account below)
#   scontrol show partition gpu            # Partition limits (max time, etc.)
################################################################################

#--- Job identity -------------------------------------------------------
#SBATCH --job-name=<PKG_NAME>

#--- Partition and account ----------------------------------------------
# "gpu" is the GPU partition on Iridis X. Verify with: sinfo -s
# Your account may differ — check with: sacctmgr show assoc user=$USER
#SBATCH --partition=gpu
#SBATCH --account=ecsstudents

#--- Node and task layout -----------------------------------------------
#SBATCH --nodes=1
#SBATCH --ntasks=1

#--- CPU cores ----------------------------------------------------------
# Number of CPU cores for your job. Your script can read this value at
# runtime via the $SLURM_CPUS_PER_TASK environment variable (e.g. for
# DataLoader num_workers).
#SBATCH --cpus-per-task=4

#--- GPU request --------------------------------------------------------
# Format: --gres=gpu:<type>:<count>
# Common GPU types on Iridis X:
#   a100   — NVIDIA A100 (80GB)
#   l4     — NVIDIA L4 (24GB, available on login node LoginX001)
# To request 2 A100s: --gres=gpu:a100:2
# To request any GPU:  --gres=gpu:1
#SBATCH --gres=gpu:a100:1

#--- Memory -------------------------------------------------------------
# RAM allocated to your job. If your job is killed with "OOM", increase
# this. Check post-run usage with: seff <job_id>
#SBATCH --mem=16G

#--- Wall time ----------------------------------------------------------
# Format: HH:MM:SS. Job is killed if it exceeds this limit.
# Training jobs may need 12-48 hours. Start conservative and check with
# seff <job_id> after a run to see actual elapsed time.
#SBATCH --time=04:00:00

#--- Output logs --------------------------------------------------------
# %j is replaced by the Slurm job ID. Logs land inside your package dir.
#SBATCH --output=<PKG_NAME>/slurm_%j.out
#SBATCH --error=<PKG_NAME>/slurm_%j.err

# --- Package Configuration ---
PKG_NAME="<PKG_NAME>"
PROJECT_DIR="${SLURM_SUBMIT_DIR:-$HOME/dpdl}"

echo "========================================="
echo " Slurm Job: $SLURM_JOB_ID"
echo " Package:   $PKG_NAME"
echo " Node:      $(hostname)"
echo " CPUs:      $SLURM_CPUS_PER_TASK"
echo " Started:   $(date)"
echo "========================================="

# --- Load container runtime ---
# Iridis X provides apptainer as a module. Older clusters may use singularity.
module load apptainer 2>/dev/null || module load singularity 2>/dev/null || {
    echo "ERROR: Neither apptainer nor singularity module available"
    echo "  Run 'module avail' to see available modules"
    exit 1
}

cd "$PROJECT_DIR"
echo "Working directory: $(pwd)"

# --- Run inside container ---
# --nv:       passes host NVIDIA drivers and GPU devices into the container
# --unsquash: required on Iridis X compute nodes (they lack fusermount)
# --bind $PWD: mounts the project directory so the container can see all packages
#
# Replace the python3 command below with your own entry point, e.g.:
#   python3 "${PKG_NAME}/train.py" --epochs 50
apptainer exec --nv --unsquash --bind "$PWD" \
    rcotformer.sif \
    python3 "${PKG_NAME}/your_script.py"

EXIT_CODE=$?

echo "========================================="
echo " Job finished: $(date)"
echo " Exit code:    $EXIT_CODE"
echo "========================================="

exit $EXIT_CODE
